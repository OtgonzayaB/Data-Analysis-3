---
title: "Assignment 2"
author: "Otgonzaya Battulga"
date: "2025-03-30"
format: html
editor: visual
---
# Building a Prediction Model on House Prices.

In this report, I have developed a pricing model to support the operation of a chain of Airbnb properties by building and evaluating machine learning models that predict listing prices based on property characteristics, host attributes, amenities, and location.

The core dataset used is the **Q3 2024 Airbnb listings for Florida**, sourced from **Inside Airbnb**. This dataset contains over 16,000 observations, providing a comprehensive snapshot of the Florida short-term rental market. Extensive data wrangling was performed, including the extraction of key amenities, handling of missing values, and transformation of categorical variables into model ready features.

**Five predictive models were developed and compared:**

- Ordinary Least Squares (OLS)
- LASSO Regression
- Random Forest
- XGBoost
- CART

Model performance was evaluated based on predictive accuracy (RMSE and R^2) and training time, summarized in a horserace table. The best-performing models, Random Forest and XGBoost were further analyzed to explore feature importance and identify key price drivers.

**To test the validity and generalizability of the models, two additional datasets were introduced:**

- A later snapshot from the same region: Florida Q4 2024
- A different market within the U.S.: Denver Q4 2024

These datasets were used to evaluate how well the models perform across time and geographic location, providing insights into their robustness and potential for wider deployment across an Airbnb portfolio.

```{r loading libraries}
#| output: false
#| echo: false
#| message: false
#| warning: false

library(readxl)
library(tidyverse)
library(caret)
library(glmnet)
library(randomForest)
library(xgboost)
library(e1071)
library(Metrics)
library(rpart)
library(rpart.plot)
library(dplyr)
library(ggplot2)
library(knitr)
library(MetBrewer)
library(scales)
library(kableExtra)
```

```{r importing the dataset}
#| echo: false
#| warning: false

# Set the path to your dataset
file_path <- "data/florida-q3-listings.xlsx"

# Read the Excel file
df <- read_excel(file_path)
```

```{r see column names}
#| output: false
#| echo: false

names(df)
```

# Part 1. Modelling

First, I have looked at the column names and chosen the attributes and amenities I want to use to build the price model. I selected variables that are most likely to influence listing prices, including listing-level characteristics (number of bedrooms, bathrooms, accommodates etc), host attributes (superhost status, identity verification etc), geographic information (latitude, longitude, and neighbourhood etc), and key amenities such as WiFi, kitchen, pool, and parking.

## 1. Data Wrangling

```{r}
#| output: false
#| warning: false

# Making sure price is numeric and has valid values
df <- df %>%
  mutate(price = as.numeric(gsub("[$,]", "", price))) %>%
  filter(!is.na(price)) %>%
  filter(price > 20 & price < 1000)

# Clean up key columns
df_model <- df %>%
  mutate(
    host_is_superhost = ifelse(host_is_superhost == "t", 1, 0),
    host_identity_verified = ifelse(host_identity_verified == "t", 1, 0),
    host_has_profile_pic = ifelse(host_has_profile_pic == "t", 1, 0),
    host_response_rate = as.numeric(gsub("%", "", host_response_rate)),
    host_acceptance_rate = as.numeric(gsub("%", "", host_acceptance_rate)),
    latitude = as.numeric(latitude),
    longitude = as.numeric(longitude)
  )
```

The price column was first cleaned by removing dollar signs and commas, and filtered to exclude extreme outliers (keeping prices between $20 and $1000). Boolean features such as host_is_superhost, host_identity_verified, and host_has_profile_pic were converted from text to binary (1/0) format. Similarly, percentage-based features like host_response_rate and host_acceptance_rate were converted to numeric.

```{r}
# Impute missing values for numerical columns
df_model <- df_model %>%
  mutate(
    bedrooms = ifelse(is.na(bedrooms), median(bedrooms, na.rm = TRUE), bedrooms),
    beds = ifelse(is.na(beds), median(beds, na.rm = TRUE), beds),
    bathrooms = ifelse(is.na(bathrooms), median(bathrooms, na.rm = TRUE), bathrooms),
    host_response_rate = ifelse(is.na(host_response_rate), median(host_response_rate, na.rm = TRUE), host_response_rate),
    host_acceptance_rate = ifelse(is.na(host_acceptance_rate), median(host_acceptance_rate, na.rm = TRUE), host_acceptance_rate),
    host_listings_count = ifelse(is.na(host_listings_count), median(host_listings_count, na.rm = TRUE), host_listings_count)
  )

# Parse amenities into binary features
df_model <- df_model %>%
  mutate(
    has_wifi = as.numeric(grepl("Wifi", amenities)),
    has_kitchen = as.numeric(grepl("Kitchen", amenities)),
    has_pool = as.numeric(grepl("Pool", amenities)),
    has_aircon = as.numeric(grepl("Air conditioning", amenities)),
    has_parking = as.numeric(grepl("Free parking", amenities)),
    has_tv = as.numeric(grepl("TV", amenities)),
    has_hot_tub = as.numeric(grepl("Hot tub", amenities)),
    has_washer = as.numeric(grepl("Washer", amenities)),
    has_dryer = as.numeric(grepl("Dryer", amenities))
  )
```

To handle missing data, I used median imputation for numerical fields such as bedrooms, bathrooms, beds, and the host-related response and acceptance rates. For the amenities, I used regular expressions to create binary indicator variables based on whether a listing includes features like "Wifi", "Kitchen", or "Pool" in its amenities string.

```{r}
#| output: false
#| warning: false

# Select relevant columns
df_model <- df_model %>%
  select(price, accommodates, bedrooms, beds, bathrooms,
         host_is_superhost, host_identity_verified, host_has_profile_pic,
         host_response_rate, host_acceptance_rate, host_listings_count,
         latitude, longitude, neighbourhood_cleansed,
         property_type, room_type,
         starts_with("has_")) %>%
  drop_na()

# Convert categorical vars to numeric
df_model <- df_model %>%
  mutate(across(c(property_type, room_type, neighbourhood_cleansed), as.factor)) %>%
  mutate(across(c(property_type, room_type, neighbourhood_cleansed), ~ as.numeric(as.factor(.x))))

# Final check
glimpse(df_model)
```

Finally, I converted categorical variables (property_type, room_type, and neighbourhood_cleansed) to numerical factors and removed any constant or missing-value-only columns to ensure compatibility with modeling.

## 2. Building Predictive Models

The cleaned and feature-engineered dataset was then split into training and testing sets, with 80% of the data used for model training and 20% for evaluation. Constant columns and those with only a single unique value were removed to prevent issues during model fitting.

```{r train-test split}
set.seed(123)
trainIndex <- createDataPartition(df_model$price, p = 0.8, list = FALSE)
train <- df_model[trainIndex, ]
test  <- df_model[-trainIndex, ]

# Separate features and target
X_train <- train %>% select(-price)
y_train <- train$price
X_test <- test %>% select(-price)
y_test <- test$price

# Remove columns with only 1 unique value
X_train <- X_train[, sapply(X_train, function(col) length(unique(col)) > 1)]
X_test <- X_test[, colnames(X_train)]  # Match test columns to training set
```

### 2.1 Model 1 - OLS

```{r Model 1: OLS}

# Dropping constant columns from train
train_filtered <- train[, sapply(train, function(x) length(unique(x)) > 1)]

#  Make sure all characters/factors are valid
train_filtered <- train_filtered %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), ~ droplevels(.)))

# Start timing
start_time_ols <- Sys.time()

# Rebuild the model
model_ols <- lm(price ~ ., data = train_filtered)

# End timing
end_time_ols <- Sys.time()
train_time_ols <- round(as.numeric(difftime(end_time_ols, start_time_ols, units = "secs")), 3)  # in seconds

# Apply same columns to test set
test_filtered <- test[, colnames(train_filtered)]

# Predicting with OLS
pred_ols <- predict(model_ols, newdata = test_filtered)
```

As the first predictive model, I implemented an OLS regression. Before training, I ensured the training data had no constant columns and that any character variables were converted to factors with valid levels. This step was necessary to avoid errors during model fitting and ensure meaningful interpretation of coefficients.

The model was trained using all available features to predict the listing price. I also measured the training time to later compare the computational efficiency of different models. The test dataset was aligned to include the same variables as the training set before making predictions.

```{r Model 1: OLS Results}

rmse_ols <- rmse(test$price, pred_ols)
r2_ols <- R2(pred_ols, test$price)


# Store results in a data frame
results_ols <- data.frame(
  Model = "OLS",
  RMSE = round(rmse(test$price, pred_ols), 2),
  R2 = round(R2(pred_ols, test$price), 4),
  Training_Time = train_time_ols
)

# Display as a table
knitr::kable(results_ols, caption = "OLS Model Performance", align = "c")
```

The OLS model achieved a RMSE of 108.87 and an R^2 of 0.4101, indicating moderate predictive power. The model was relatively fast to train, completing in approximately 0.039 seconds. While OLS is simple and interpretable, its ability to capture complex relationships is limited compared to non-linear models.

### 2.2. Model 2 - LASSO

```{r Model 2: LASSO}

# Convert data to matrix format
X_train_mat <- model.matrix(price ~ ., data = train_filtered)[, -1]  # Remove intercept
X_test_mat  <- model.matrix(price ~ ., data = test_filtered)[, -1]
y_train_vec <- train_filtered$price

# Track training time
start_time_lasso <- Sys.time()

# Train LASSO model with cross-validation
lasso_cv <- cv.glmnet(X_train_mat, y_train_vec, alpha = 1)

end_time_lasso <- Sys.time()
train_time_lasso <- round(as.numeric(difftime(end_time_lasso, start_time_lasso, units = "secs")), 3)

# Predict on test set using best lambda
pred_lasso <- predict(lasso_cv, s = "lambda.min", newx = X_test_mat)

# Evaluate
rmse_lasso <- rmse(test$price, pred_lasso)
r2_lasso <- R2(pred_lasso, test$price)

# Create results table
results_lasso <- data.frame(
  Model = "LASSO",
  RMSE = round(rmse_lasso, 2),
  R2 = round(as.numeric(r2_lasso), 4),
  Training_Time = train_time_lasso
)

# Display results
knitr::kable(results_lasso, caption = "LASSO Model Performance", align = "c")
```

For the second model, I applied LASSO regression, which is particularly effective in high-dimensional settings and for automatic feature selection. Unlike OLS, LASSO adds a penalty term that can shrink some coefficients to exactly zero, effectively removing less informative predictors.

Before training, the data was converted to a matrix format compatible with the glmnet package. I used cross-validation to select the optimal value of the regularization parameter (lambda.min). Training time was also recorded for later comparison.

The LASSO model achieved a RMSE of 108.87 and an R^2 of 0.4102, which is nearly identical to the performance of the OLS model. However, LASSO provides the additional benefit of regularization, potentially improving generalization on unseen data. The model trained in 0.213 seconds, showing slightly higher computational cost compared to OLS due to the cross-validation step.

### 2.3. Model 3 - Random Forest

```{r Model 3: Random Forest}

# Track training time
start_time_rf <- Sys.time()

# Train model
set.seed(123)
model_rf <- randomForest(price ~ ., data = train_filtered, ntree = 100, importance = TRUE)

end_time_rf <- Sys.time()
train_time_rf <- round(as.numeric(difftime(end_time_rf, start_time_rf, units = "secs")), 3)

# Predict
pred_rf <- predict(model_rf, newdata = test_filtered)

# Evaluate
rmse_rf <- rmse(test$price, pred_rf)
r2_rf <- R2(pred_rf, test$price)

# Create results table
results_rf <- data.frame(
  Model = "Random Forest",
  RMSE = round(rmse_rf, 2),
  R2 = round(as.numeric(r2_rf), 4),
  Training_Time = train_time_rf
)

# Display
knitr::kable(results_rf, caption = "Random Forest Model Performance", align = "c")
```

The third model I implemented was a Random Forest, a powerful ensemble learning method that builds multiple decision trees and aggregates their predictions to improve accuracy and reduce overfitting. It is well-suited for capturing non-linear relationships and complex feature interactions.

The model was trained using 100 trees (ntree = 100) and configured to calculate feature importance. I recorded the training time for comparison with the other models.

Random Forest significantly outperformed both OLS and LASSO, achieving an RMSE of 87.03 and an R^2 of 0.6232 on the test data. This indicates a notable improvement in predictive power. However, this came at the cost of longer training time of approximately 13.917 seconds, which is considerably higher than the previous models due to the ensemble nature of the algorithm.

### 2.4. Model 4 - XGBoost

```{r Model 4: XGBoost}

# Prepare matrix for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dtest  <- xgb.DMatrix(data = as.matrix(X_test))

# Define parameters
params <- list(
  objective = "reg:squarederror",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Track training time
set.seed(123)
start_time_xgb <- Sys.time()

model_xgb <- xgboost(
  params = params,
  data = dtrain,
  nrounds = 100,
  verbose = 0
)

end_time_xgb <- Sys.time()
train_time_xgb <- round(as.numeric(difftime(end_time_xgb, start_time_xgb, units = "secs")), 3)

# Predict
pred_xgb <- predict(model_xgb, newdata = dtest)

# Evaluate
rmse_xgb <- rmse(y_test, pred_xgb)
r2_xgb <- R2(pred_xgb, y_test)

# Create table
results_xgb <- data.frame(
  Model = "XGBoost",
  RMSE = round(rmse_xgb, 2),
  R2 = round(as.numeric(r2_xgb), 4),
  Training_Time = train_time_xgb
)

# Display table
knitr::kable(results_xgb, caption = "XGBoost Model Performance", align = "c")
```

